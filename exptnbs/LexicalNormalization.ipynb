{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalNormalization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkolachi/lexicalnormalization/blob/master/exptnbs/LexicalNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOoroVGYw3Mg"
      },
      "source": [
        "[WNUT21 Shared Task Website](http://noisy-text.github.io/2021/multi-lexnorm.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRtzY3ApXhe"
      },
      "source": [
        "### Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywhY3ZLXJs0P",
        "outputId": "636b8148-d9c3-4f86-855e-84d73ef71c88"
      },
      "source": [
        "!git clone https://github.com/pkolachi/lexicalnormalization\n",
        "%pip install --user -U pandas==1.1.5\n",
        "%pip install --user -U scikit-learn==0.22.2.post1\n",
        "%pip install --user -U sklearn-crfsuite"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lexicalnormalization'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 96 (delta 14), reused 59 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n",
            "Requirement already up-to-date: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.0.1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkdFPXmKBCH"
      },
      "source": [
        "REPO_NAME = 'lexicalnormalization'\n",
        "LANGS = {'da': 'Danish',\n",
        "         'en': 'English',\n",
        "         'es': 'Spanish',\n",
        "         'hr': 'Croatian',\n",
        "         'iden': 'Indonesian-English',\n",
        "         'it': 'Italian',\n",
        "         'nl': 'Dutch',\n",
        "         'sl': 'Slovenian',\n",
        "         'sr': 'Serbian',\n",
        "         'tr': 'Turkish',\n",
        "         'trde': 'Turkish-German',\n",
        "         }\n",
        "SMPLS = LANGS.keys()\n",
        "EMPTY_TOKEN = '+-#EMPTOK#-+'\n",
        "EMPTY_LABEL = '+-#MERGE#-+'  #''\n",
        "PAD_TOKEN   = '+-#NULL#-+'\n",
        "PAD_LABEL   = '+-#DROP#-+'\n",
        "TST_RATIO = 0.15     # use 15% of training data as held-out data for evaluation\n",
        "CVFOLDS   = 4        # use 4-folds for cross-fold training throughout experiments"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzppLsjpdql"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTSBr-AMNXzf"
      },
      "source": [
        "from collections import defaultdict \n",
        "import itertools as it \n",
        "from operator import itemgetter\n",
        "import os.path \n",
        "import pandas as pd\n",
        "\n",
        "def load_data(inpfile, empty_label=''):\n",
        "  with open(inpfile) as inf:\n",
        "    # break lines into sentence blocks\n",
        "    snb = (list(it.takewhile(lambda lne: lne.strip(), inf)) for _ in it.count(1))\n",
        "    # deal with errors in file format especially turkish\n",
        "    snc = it.dropwhile(lambda snt: len(snt) == 0, snb)\n",
        "    # terminate this infinite stream \n",
        "    snd = it.takewhile(lambda snt: len(snt)  > 0, snc)\n",
        "    # split into fields\n",
        "    crs = ([t.strip('\\n').split('\\t', 1) for t in s] for s in snd)\n",
        "    if empty_label:\n",
        "      crp = [[(tok[0], tok[1] if len(tok) > 1 and tok[1].strip() else empty_label)\n",
        "              for tok in sent]\n",
        "             for sent in crs]\n",
        "    else:\n",
        "      crp = list(crs)\n",
        "    return crp\n",
        "\n",
        "# remove sentences that do not follow the expected format\n",
        "sanitize_crps = lambda sent: all(len(fields) == 2 for fields in sent)\n",
        "# get input from tuple (raw sentences)\n",
        "get_rawtokens = lambda sent: list(map(itemgetter(0), sent)) \n",
        "# get output/labels from tuple (normalized sentences)\n",
        "get_nrmtokens = lambda sent: list(map(itemgetter(1), sent))\n",
        "\n",
        "DATA = defaultdict(lambda: defaultdict(lambda: ([], [])))\n",
        "for lang in SMPLS:\n",
        "  datadir = os.path.join(REPO_NAME, 'data', lang)\n",
        "  trnfile = os.path.join(datadir, 'train.norm')\n",
        "  devfile = os.path.join(datadir, 'dev.norm')\n",
        "  tstfile = os.path.join(datadir, 'test.norm')\n",
        "  for dts, dtf in [('fulltrn', trnfile), ('dev', devfile), ('tst', tstfile)]:\n",
        "    if os.path.isdir(datadir) and os.path.isfile(dtf):\n",
        "      ocrp = list(load_data(dtf, empty_label=EMPTY_LABEL))\n",
        "      # sanitize corpus to make sure\n",
        "      fcrp = list(filter(sanitize_crps, ocrp))\n",
        "      if len(ocrp) != len(fcrp): print(\"Removed {0} sentences from {1}\".format(len(ocrp)-len(fcrp), dtf))\n",
        "      X = list(map(get_rawtokens, fcrp))  \n",
        "      Y = list(map(get_nrmtokens, fcrp))\n",
        "      DATA[lang][dts] = (X, Y)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkCFpXbrwD5U"
      },
      "source": [
        "%rm -rf $REPO_NAME"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEmm52m_3Gzo"
      },
      "source": [
        "### Data statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4J-9JaRXMOX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "for lang in SMPLS:\n",
        "  if 'fulltrn' in DATA[lang]:\n",
        "    trn_x, hld_x, trn_y, hld_y = train_test_split(DATA[lang]['fulltrn'][0], \n",
        "                                                  DATA[lang]['fulltrn'][1], \n",
        "                                                  test_size=TST_RATIO, \n",
        "                                                  random_state=0, \n",
        "                                                  shuffle=False)\n",
        "    DATA[lang]['trn'] = (trn_x, trn_y)\n",
        "    DATA[lang]['hld'] = (hld_x, hld_y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "7nz69Y8xwD5U",
        "outputId": "4a0e61cd-140a-4446-800c-a10da5d0f924"
      },
      "source": [
        "columns = ['Language', 'Training', 'Held-out', 'Devel', 'Test']\n",
        "datasizes = [[LANGS[lang]]+[len(DATA[lang][crp][0]) \n",
        "                     for crp in ('trn', 'hld', 'dev', 'tst')] \n",
        "             for lang in SMPLS]\n",
        "datasizes = pd.DataFrame.from_records(datasizes, columns=columns)\n",
        "\n",
        "datasizes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Training</th>\n",
              "      <th>Held-out</th>\n",
              "      <th>Devel</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Danish</td>\n",
              "      <td>175</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>English</td>\n",
              "      <td>2006</td>\n",
              "      <td>354</td>\n",
              "      <td>590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>482</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>381</td>\n",
              "      <td>68</td>\n",
              "      <td>1588</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indonesian-English</td>\n",
              "      <td>420</td>\n",
              "      <td>75</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Italian</td>\n",
              "      <td>504</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dutch</td>\n",
              "      <td>798</td>\n",
              "      <td>141</td>\n",
              "      <td>314</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Slovenian</td>\n",
              "      <td>3969</td>\n",
              "      <td>701</td>\n",
              "      <td>1557</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Serbian</td>\n",
              "      <td>3517</td>\n",
              "      <td>621</td>\n",
              "      <td>1327</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Turkish</td>\n",
              "      <td>484</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Turkish-German</td>\n",
              "      <td>680</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Language  Training  Held-out  Devel  Test\n",
              "0               Danish       175        32      0     0\n",
              "1              English      2006       354    590     0\n",
              "2              Spanish       482        86      0     0\n",
              "3             Croatian       381        68   1588     0\n",
              "4   Indonesian-English       420        75    165     0\n",
              "5              Italian       504        89      0     0\n",
              "6                Dutch       798       141    314     0\n",
              "7            Slovenian      3969       701   1557     0\n",
              "8              Serbian      3517       621   1327     0\n",
              "9              Turkish       484        86      0     0\n",
              "10      Turkish-German       680       120      0     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "PC8E_x4F3TTu",
        "outputId": "fd2468df-6be4-410d-b0b1-904053b20d49"
      },
      "source": [
        "datasizes['AllToks#'] = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                 for typ in (0, 1)\n",
        "                                 for sent in DATA[lang][dts][typ]\n",
        "                                 for tok in sent)) for lang in SMPLS]\n",
        "\n",
        "datasizes['Vocab#']  = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                for sent in DATA[lang][dts][0] for tok in sent))\n",
        "                        for lang in SMPLS]\n",
        "datasizes['Labels#'] = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                for sent in DATA[lang][dts][1] for tok in sent))\n",
        "                        for lang in SMPLS]\n",
        "\n",
        "datasizes['Trn. Vocab#'] = [len(set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                    for tok in sent)) for lang in SMPLS]\n",
        "datasizes['Trn. Label#'] = [len(set(tok for sent in DATA[lang]['trn'][1]\n",
        "                                    for tok in sent)) for lang in SMPLS]\n",
        "\n",
        "datasizes['Ood. Vocab%'] = [len(set(tok for dts in ('hld', 'dev', 'tst')\n",
        "                                    for sent in DATA[lang][dts][0]\n",
        "                                    for tok in sent).difference(\n",
        "                                        set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                            for tok in sent)\n",
        "                                        )) for lang in SMPLS]\n",
        "datasizes['Ood. Label%'] = [len(set(tok for dts in ('hld', 'dev', 'tst')\n",
        "                                    for sent in DATA[lang][dts][1]\n",
        "                                    for tok in sent).difference(\n",
        "                                        set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                            for tok in sent)\n",
        "                                        )) for lang in SMPLS]\n",
        "datasizes['Ood. Vocab%'] = 100 * datasizes['Ood. Vocab%'] / datasizes['Vocab#']\n",
        "datasizes['Ood. Label%'] = 100 * datasizes['Ood. Label%'] / datasizes['Labels#']\n",
        "\n",
        "datasizes "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Training</th>\n",
              "      <th>Held-out</th>\n",
              "      <th>Devel</th>\n",
              "      <th>Test</th>\n",
              "      <th>AllToks#</th>\n",
              "      <th>Vocab#</th>\n",
              "      <th>Labels#</th>\n",
              "      <th>Trn. Vocab#</th>\n",
              "      <th>Trn. Label#</th>\n",
              "      <th>Ood. Vocab%</th>\n",
              "      <th>Ood. Label%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Danish</td>\n",
              "      <td>175</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3421</td>\n",
              "      <td>3333</td>\n",
              "      <td>3280</td>\n",
              "      <td>2935</td>\n",
              "      <td>2886</td>\n",
              "      <td>11.941194</td>\n",
              "      <td>12.195122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>English</td>\n",
              "      <td>2006</td>\n",
              "      <td>354</td>\n",
              "      <td>590</td>\n",
              "      <td>0</td>\n",
              "      <td>13437</td>\n",
              "      <td>13127</td>\n",
              "      <td>12521</td>\n",
              "      <td>9589</td>\n",
              "      <td>9130</td>\n",
              "      <td>26.952083</td>\n",
              "      <td>27.561696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>482</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3285</td>\n",
              "      <td>3064</td>\n",
              "      <td>2833</td>\n",
              "      <td>2695</td>\n",
              "      <td>2504</td>\n",
              "      <td>12.043081</td>\n",
              "      <td>12.072008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>381</td>\n",
              "      <td>68</td>\n",
              "      <td>1588</td>\n",
              "      <td>0</td>\n",
              "      <td>9609</td>\n",
              "      <td>8886</td>\n",
              "      <td>8434</td>\n",
              "      <td>2110</td>\n",
              "      <td>2085</td>\n",
              "      <td>76.254783</td>\n",
              "      <td>75.492056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indonesian-English</td>\n",
              "      <td>420</td>\n",
              "      <td>75</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>6185</td>\n",
              "      <td>5754</td>\n",
              "      <td>5284</td>\n",
              "      <td>4256</td>\n",
              "      <td>3887</td>\n",
              "      <td>26.034063</td>\n",
              "      <td>27.346707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Italian</td>\n",
              "      <td>504</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4941</td>\n",
              "      <td>4619</td>\n",
              "      <td>4383</td>\n",
              "      <td>4046</td>\n",
              "      <td>3836</td>\n",
              "      <td>12.405283</td>\n",
              "      <td>12.616929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dutch</td>\n",
              "      <td>798</td>\n",
              "      <td>141</td>\n",
              "      <td>314</td>\n",
              "      <td>0</td>\n",
              "      <td>7885</td>\n",
              "      <td>6651</td>\n",
              "      <td>5394</td>\n",
              "      <td>3591</td>\n",
              "      <td>2902</td>\n",
              "      <td>46.008119</td>\n",
              "      <td>49.536522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Slovenian</td>\n",
              "      <td>3969</td>\n",
              "      <td>701</td>\n",
              "      <td>1557</td>\n",
              "      <td>0</td>\n",
              "      <td>17673</td>\n",
              "      <td>15963</td>\n",
              "      <td>14501</td>\n",
              "      <td>11229</td>\n",
              "      <td>10276</td>\n",
              "      <td>29.656080</td>\n",
              "      <td>30.315151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Serbian</td>\n",
              "      <td>3517</td>\n",
              "      <td>621</td>\n",
              "      <td>1327</td>\n",
              "      <td>0</td>\n",
              "      <td>20385</td>\n",
              "      <td>18658</td>\n",
              "      <td>17443</td>\n",
              "      <td>13705</td>\n",
              "      <td>12912</td>\n",
              "      <td>26.546254</td>\n",
              "      <td>27.019435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Turkish</td>\n",
              "      <td>484</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5837</td>\n",
              "      <td>4326</td>\n",
              "      <td>3992</td>\n",
              "      <td>3760</td>\n",
              "      <td>3501</td>\n",
              "      <td>13.083680</td>\n",
              "      <td>13.677355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Turkish-German</td>\n",
              "      <td>680</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7856</td>\n",
              "      <td>6156</td>\n",
              "      <td>5752</td>\n",
              "      <td>5514</td>\n",
              "      <td>5194</td>\n",
              "      <td>10.428850</td>\n",
              "      <td>10.639777</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Language  Training  ...  Ood. Vocab%  Ood. Label%\n",
              "0               Danish       175  ...    11.941194    12.195122\n",
              "1              English      2006  ...    26.952083    27.561696\n",
              "2              Spanish       482  ...    12.043081    12.072008\n",
              "3             Croatian       381  ...    76.254783    75.492056\n",
              "4   Indonesian-English       420  ...    26.034063    27.346707\n",
              "5              Italian       504  ...    12.405283    12.616929\n",
              "6                Dutch       798  ...    46.008119    49.536522\n",
              "7            Slovenian      3969  ...    29.656080    30.315151\n",
              "8              Serbian      3517  ...    26.546254    27.019435\n",
              "9              Turkish       484  ...    13.083680    13.677355\n",
              "10      Turkish-German       680  ...    10.428850    10.639777\n",
              "\n",
              "[11 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRbS1VJ396r2"
      },
      "source": [
        "### Task Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omrn2mZv7bT9"
      },
      "source": [
        "from sklearn.model_selection import KFold, RepeatedKFold\n",
        "\n",
        "FOLDS = defaultdict(list)\n",
        "kcvs = RepeatedKFold(n_splits=CVFOLDS, n_repeats=5, random_state=0)\n",
        "for lang in SMPLS:\n",
        "  FOLDS[lang].extend(kcvs.split(DATA[lang]['trn'][0]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwPn_NHdKG6g"
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np \n",
        "\n",
        "# Leave-As-Is i.e. return input as output \n",
        "class LAI(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, pad_tok=PAD_TOKEN, pad_lbl=PAD_LABEL):\n",
        "    self.__pad_tok = pad_tok\n",
        "    self.__pad_lbl = pad_lbl\n",
        "    self.__scores = defaultdict(float)\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    max_len = max(it.chain((len(seq) for seq in X), (len(seq) for seq in Y)))\n",
        "    #X = np.asarray([seq + [self.__pad_tok]*(max_len-len(seq)) for seq in X], dtype=str)\n",
        "    #Y = np.asarray([seq + [self.__pad_lbl]*(max_len-len(seq)) for seq in Y], dtype=str)\n",
        "    return\n",
        "\n",
        "  def predict(self, X):\n",
        "    max_len = max(len(seq) for seq in X)\n",
        "    #X = np.asarray([seq + [self.__pad_tok]*(max_len-len(seq)) for seq in X], dtype=str)\n",
        "    return X\n",
        "\n",
        "  def score(self, X, Y, ignoreCase=False):\n",
        "    prdY = self.predict(X)\n",
        "    zipS = ((inp, out, oup) for inp, out, oup in zip(X, Y, prdY))\n",
        "    # eliminate instances if lengths do not match\n",
        "    zipF = (seq for seq in zipS if len(seq[1]) == len(seq[2]))\n",
        "    tokS = ((rawW, gldW, prdW) for seq in zipF for rawW, gldW, prdW in zip(*seq))\n",
        "    correct, changed, total = 0, 0, 0\n",
        "    for rawW, gldW, prdW in tokS:\n",
        "      total += 1 \n",
        "      if ignoreCase:\n",
        "        rawW = rawW.lower()\n",
        "        gldW = gldW.lower()\n",
        "        prdW = prdW.lower()\n",
        "      if rawW != gldW:\n",
        "        changed += 1\n",
        "      if gldW == prdW:\n",
        "        correct += 1\n",
        "    # evaluation used in the shared task \n",
        "    self.__scores['accuracy'] = correct/total\n",
        "    self.__scores['lai'] = (total-changed)/total\n",
        "    if self.__scores['lai'] == 1:\n",
        "      self.__scores['err'] = 0\n",
        "    else:\n",
        "      self.__scores['err'] = (self.__scores['accuracy']\n",
        "                              -self.__scores['lai'])/(1-self.__scores['lai'])\n",
        "    return self.__scores['accuracy']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTZOImW997A-"
      },
      "source": [
        "# Most-Frequent-Replacement\n",
        "class MFR(LAI):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.__counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    super().fit(X, Y)\n",
        "    replacements = ((itok, otok) for iseg, oseg in zip(X, Y)\n",
        "                                   for itok, otok in zip(iseg, oseg))\n",
        "    for inp, rpl in replacements:\n",
        "      self.__counts[inp][rpl] += 1 \n",
        "  \n",
        "  def predict(self, X):\n",
        "    X = super().predict(X)\n",
        "    prdY = [] \n",
        "    for iseg in X:\n",
        "      oseg = []\n",
        "      for itok in iseg:\n",
        "        lns = self.__counts[itok]\n",
        "        if len(lns) == 0: \n",
        "          oseg.append(itok)\n",
        "        else:\n",
        "          oseg.append(sorted(lns.items(), key=itemgetter(1))[0][0])\n",
        "      prdY.append(oseg)\n",
        "    return prdY"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFV_O4WAFRoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc215fa1-9c58-441c-e223-1309e3ae23fb"
      },
      "source": [
        "from sklearn.model_selection import cross_validate \n",
        "\n",
        "for lang in SMPLS:\n",
        "  cvres = cross_validate(LAI(), DATA[lang]['trn'][0], DATA[lang]['trn'][1], \n",
        "                         cv=FOLDS[lang], \n",
        "                         return_train_score=True)\n",
        "  print(lang, sum(acc for acc in cvres['test_score'])/len(cvres['test_score']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "da 0.9773000655627598\n",
            "en 0.9232738531092831\n",
            "es 0.9246581723460154\n",
            "hr 0.9669067116127978\n",
            "iden 0.8485335907614499\n",
            "it 0.9271346731539662\n",
            "nl 0.6497674251907003\n",
            "sl 0.8539241315573796\n",
            "sr 0.9230841306394515\n",
            "tr 0.6415626899165732\n",
            "trde 0.7741654409101419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUyx0JVEMnj"
      },
      "source": [
        "### Sequence classification using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdjItmlUpj3k"
      },
      "source": [
        "##### Preprocessing\n",
        "\n",
        "In this step, we convert the sequence of tokens into an embedding matrix. \n",
        "This step relies on tokenizers and pre-trained models from ``huggingface``.\n",
        "This seperation of preprocessing should allow for training other classifiers \n",
        "than neural versions using ``scikit-learn`` or other packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qkRzWpGY-fW"
      },
      "source": [
        "for lang in SMPLS:\n",
        "  # load tokenizer model from ``huggingface``\n",
        "  for dts in DATA[lang]:\n",
        "    for inp, out in zip(DATA[lang][dts][0], DATA[lang][dts][1]):\n",
        "      tinp = []\n",
        "      tout = []\n",
        "      for intok, outok in zip(inp, out):\n",
        "        pass"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0rYO-eeEMy-"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5arwc2T385K"
      },
      "source": [
        "### Sequence classification using HMM models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtNuRD1539GR"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}