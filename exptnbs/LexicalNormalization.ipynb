{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalNormalization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkolachi/lexicalnormalization/blob/master/exptnbs/LexicalNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOoroVGYw3Mg"
      },
      "source": [
        "[WNUT21 Shared Task Website](http://noisy-text.github.io/2021/multi-lexnorm.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRtzY3ApXhe"
      },
      "source": [
        "### Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywhY3ZLXJs0P",
        "outputId": "5400a694-1b2f-4c53-b252-86aaadd7c2cd"
      },
      "source": [
        "!git clone https://github.com/pkolachi/lexicalnormalization\n",
        "%pip install --user -U pandas==1.1.5\n",
        "%pip install --user -U scikit-learn==0.22.2.post1\n",
        "%pip install --user -U sklearn-crfsuite"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lexicalnormalization' already exists and is not an empty directory.\n",
            "Requirement already up-to-date: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.4.1)\n",
            "Requirement already up-to-date: sklearn-crfsuite in /root/.local/lib/python3.7/site-packages (0.3.6)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: python-crfsuite>=0.8.3 in /root/.local/lib/python3.7/site-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkdFPXmKBCH"
      },
      "source": [
        "REPO_NAME = 'lexicalnormalization'\n",
        "LANGS = {'da': 'Danish',\n",
        "         'en': 'English',\n",
        "         'es': 'Spanish',\n",
        "         'hr': 'Croatian',\n",
        "         'iden': 'Indonesian-English',\n",
        "         'it': 'Italian',\n",
        "         'nl': 'Dutch',\n",
        "         'sl': 'Slovenian',\n",
        "         'sr': 'Serbian',\n",
        "         'tr': 'Turkish',\n",
        "         'trde': 'Turkish-German',\n",
        "         }\n",
        "SMPLS = LANGS.keys()\n",
        "EMPTY_LABEL = '+-#MERGE#-+' #''\n",
        "PAD_LABEL   = '+-#DROP#-+'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzppLsjpdql"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTSBr-AMNXzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0129f2-82a3-4021-dab6-ab038379b762"
      },
      "source": [
        "from collections import defaultdict \n",
        "import itertools as it \n",
        "from operator import itemgetter\n",
        "import os.path \n",
        "import pandas as pd\n",
        "\n",
        "def load_data_v0(inpfile, empty_label=''): \n",
        "  with open(inpfile) as ins:\n",
        "    sent = []\n",
        "    for lne in ins:\n",
        "      if not lne.strip() and len(sent):\n",
        "        yield sent\n",
        "        sent = []\n",
        "      else:\n",
        "        fields = tuple(lne.strip('\\n').split('\\t', 1))\n",
        "        if len(fields) > 1 and not fields[1].strip() and empty_label:\n",
        "          fields = (fields[0], empty_label)\n",
        "        sent.append(fields)\n",
        "    # this shouldn't be necessary if the files are correctly formatted\n",
        "    # but if EOF is encountered without a blank line at the end\n",
        "    if len(sent):\n",
        "      yield sent\n",
        "      sent = []\n",
        "\n",
        "def load_data(inpfile, empty_label=''):\n",
        "  with open(inpfile) as inf:\n",
        "    # break lines into sentence blocks\n",
        "    snb = (list(it.takewhile(lambda lne: lne.strip(), inf)) for _ in it.count(1))\n",
        "    # deal with errors in file format especially turkish\n",
        "    snc = it.dropwhile(lambda snt: len(snt) == 0 or\n",
        "                       (len(snt) == 1 and not snt.strip()), \n",
        "                      snb)\n",
        "    # terminate this infinite stream \n",
        "    snd = it.takewhile(lambda snt: len(snt)  > 0, snc)\n",
        "    # deal with errors in file format where sentences are empty\n",
        "    #snf = filter(lambda snt: not (len(snt) == 1 and snt[0].strip() == ''), snd)\n",
        "    # split into fields\n",
        "    crs = ([t.strip('\\n').split('\\t', 1) for t in s] for s in snd)\n",
        "    if empty_label:\n",
        "      crp = [[(tok[0], tok[1] if len(tok) > 1 and tok[1].strip() else empty_label)\n",
        "              for tok in sent]\n",
        "             for sent in crs]\n",
        "    else:\n",
        "      crp = list(crs)\n",
        "    return crp\n",
        "\n",
        "# remove sentences that do not follow the expected format\n",
        "sanitize_crps = lambda sent: all(len(fields) == 2 for fields in sent)\n",
        "# get input from tuple (raw sentences)\n",
        "get_rawtokens = lambda sent: list(map(itemgetter(0), sent)) \n",
        "# get output/labels from tuple (normalized sentences)\n",
        "get_nrmtokens = lambda sent: list(map(itemgetter(1), sent))\n",
        "\n",
        "DATA = defaultdict(lambda: defaultdict(lambda: ([], [])))\n",
        "for lang in SMPLS:\n",
        "  datadir = os.path.join(REPO_NAME, 'data', lang)\n",
        "  trnfile = os.path.join(datadir, 'train.norm')\n",
        "  devfile = os.path.join(datadir, 'dev.norm')\n",
        "  tstfile = os.path.join(datadir, 'test.norm')\n",
        "  for dts, dtf in [('fulltrn', trnfile), ('dev', devfile), ('tst', tstfile)]:\n",
        "    if os.path.isdir(datadir) and os.path.isfile(dtf):\n",
        "      ocrp0 = list(load_data_v0(dtf, empty_label=EMPTY_LABEL))\n",
        "      ocrp = list(load_data(dtf, empty_label=EMPTY_LABEL))\n",
        "      if any(s1 != s2 for s1, s2 in zip(ocrp0, ocrp)):\n",
        "        print('Corpus reader functions do not match')\n",
        "      # sanitize corpus to make sure\n",
        "      fcrp = list(filter(sanitize_crps, ocrp))\n",
        "      if len(ocrp) != len(fcrp): print(\"Removed {0} sentences from {1}\".format(len(ocrp)-len(fcrp), dtf))\n",
        "      X = list(map(get_rawtokens, fcrp))  \n",
        "      Y = list(map(get_nrmtokens, fcrp)) \n",
        "      print(dtf, len(ocrp), len(fcrp), len(ocrp0))\n",
        "      DATA[lang][dts] = (X, Y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lexicalnormalization/data/da/train.norm 207 207 207\n",
            "lexicalnormalization/data/en/train.norm 2360 2360 2360\n",
            "lexicalnormalization/data/en/dev.norm 590 590 590\n",
            "lexicalnormalization/data/es/train.norm 568 568 568\n",
            "lexicalnormalization/data/hr/train.norm 449 449 4762\n",
            "lexicalnormalization/data/hr/dev.norm 1588 1588 1588\n",
            "lexicalnormalization/data/iden/train.norm 495 495 495\n",
            "lexicalnormalization/data/iden/dev.norm 165 165 165\n",
            "lexicalnormalization/data/it/train.norm 593 593 593\n",
            "Corpus reader functions do not match\n",
            "lexicalnormalization/data/nl/train.norm 939 939 939\n",
            "lexicalnormalization/data/nl/dev.norm 314 314 314\n",
            "lexicalnormalization/data/sl/train.norm 4670 4670 4670\n",
            "lexicalnormalization/data/sl/dev.norm 1557 1557 1557\n",
            "lexicalnormalization/data/sr/train.norm 4138 4138 4138\n",
            "lexicalnormalization/data/sr/dev.norm 1327 1327 1380\n",
            "Corpus reader functions do not match\n",
            "lexicalnormalization/data/tr/train.norm 570 570 571\n",
            "lexicalnormalization/data/trde/train.norm 800 800 800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A34-0L5wl0hl",
        "outputId": "b8df32e2-54fe-4445-e94e-3ff1e1b85eb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fls = ('lexicalnormalization/data/nl/train.norm', \\\n",
        "       'lexicalnormalization/data/tr/train.norm')\n",
        "for f in fls:\n",
        "  ocrp0 = list(load_data_v0(f, empty_label=EMPTY_LABEL))\n",
        "  ocrp1 = list(load_data(f, empty_label=EMPTY_LABEL))\n",
        "  diff1 = [(i,s) for i,s in enumerate(ocrp1) if s not in ocrp0]\n",
        "  diff2 = [(i,s) for i,s in enumerate(ocrp0) if s not in ocrp1]\n",
        "  for i,s in diff1:\n",
        "    print(i, s)\n",
        "  for i,s in diff2:\n",
        "    print(i, s)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "796 [('@DeJakke', '@DeJakke'), ('.', '.'), ('Thks', 'Thks'), ('.', '.'), ('Opmerking', 'Opmerking'), ('terecht', 'terecht'), ('.', '.'), ('kgebruikte', 'Ik gebruikte'), ('minder-validen', 'minder-validen'), ('omdat', 'omdat'), ('organisatoren', 'organisatoren'), ('zich', 'zich'), ('Somival', 'Somival'), ('noemen', 'noemen'), ('.', '.'), ('Sport', 'Sport'), ('en', 'en'), ('ontspanning', 'ontspanning'), ('minder-validen', 'minder-validen'), ('.', '+-#MERGE#-+')]\n",
            "796 [('@DeJakke', '@DeJakke'), ('.', '.'), ('Thks', 'Thks'), ('.', '.'), ('Opmerking', 'Opmerking'), ('terecht', 'terecht'), ('.', '.'), ('kgebruikte', 'Ik gebruikte'), ('minder-validen', 'minder-validen'), ('omdat', 'omdat'), ('organisatoren', 'organisatoren'), ('zich', 'zich'), ('Somival', 'Somival'), ('noemen', 'noemen'), ('.', '.'), ('Sport', 'Sport'), ('en', 'en'), ('ontspanning', 'ontspanning'), ('minder-validen', 'minder-validen'), ('.',)]\n",
            "0 [('',)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkCFpXbrwD5U"
      },
      "source": [
        "#%rm -rf $REPO_NAME"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEmm52m_3Gzo"
      },
      "source": [
        "### Data statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4J-9JaRXMOX"
      },
      "source": [
        "TST_RATIO = 0.15\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "for lang in SMPLS:\n",
        "  if 'fulltrn' in DATA[lang]:\n",
        "    trn_x, hld_x, trn_y, hld_y = train_test_split(DATA[lang]['fulltrn'][0], \n",
        "                                                  DATA[lang]['fulltrn'][1], \n",
        "                                                  test_size=TST_RATIO, \n",
        "                                                  random_state=0, \n",
        "                                                  shuffle=False)\n",
        "    DATA[lang]['trn'] = (trn_x, trn_y)\n",
        "    DATA[lang]['hld'] = (hld_x, hld_y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "7nz69Y8xwD5U",
        "outputId": "aaac7e0e-35c3-4ca9-bef1-4f31b7731542"
      },
      "source": [
        "columns = ['Language', 'Training', 'Held-out', 'Devel', 'Test']\n",
        "datasizes = [[LANGS[lang]]+[len(DATA[lang][crp][0]) \n",
        "                     for crp in ('trn', 'hld', 'dev', 'tst')] \n",
        "             for lang in SMPLS]\n",
        "datasizes = pd.DataFrame.from_records(datasizes, columns=columns)\n",
        "\n",
        "datasizes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Training</th>\n",
              "      <th>Held-out</th>\n",
              "      <th>Devel</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Danish</td>\n",
              "      <td>175</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>English</td>\n",
              "      <td>2006</td>\n",
              "      <td>354</td>\n",
              "      <td>590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>482</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>381</td>\n",
              "      <td>68</td>\n",
              "      <td>1588</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indonesian-English</td>\n",
              "      <td>420</td>\n",
              "      <td>75</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Italian</td>\n",
              "      <td>504</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dutch</td>\n",
              "      <td>798</td>\n",
              "      <td>141</td>\n",
              "      <td>314</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Slovenian</td>\n",
              "      <td>3969</td>\n",
              "      <td>701</td>\n",
              "      <td>1557</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Serbian</td>\n",
              "      <td>3517</td>\n",
              "      <td>621</td>\n",
              "      <td>1327</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Turkish</td>\n",
              "      <td>484</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Turkish-German</td>\n",
              "      <td>680</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Language  Training  Held-out  Devel  Test\n",
              "0               Danish       175        32      0     0\n",
              "1              English      2006       354    590     0\n",
              "2              Spanish       482        86      0     0\n",
              "3             Croatian       381        68   1588     0\n",
              "4   Indonesian-English       420        75    165     0\n",
              "5              Italian       504        89      0     0\n",
              "6                Dutch       798       141    314     0\n",
              "7            Slovenian      3969       701   1557     0\n",
              "8              Serbian      3517       621   1327     0\n",
              "9              Turkish       484        86      0     0\n",
              "10      Turkish-German       680       120      0     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "PC8E_x4F3TTu",
        "outputId": "f72c631a-6be6-4ded-873d-87a5aa334698"
      },
      "source": [
        "datasizes['AllToks#'] = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                 for typ in (0, 1)\n",
        "                                 for sent in DATA[lang][dts][typ]\n",
        "                                 for tok in sent)) for lang in SMPLS]\n",
        "\n",
        "datasizes['Vocab#']  = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                for sent in DATA[lang][dts][0] for tok in sent))\n",
        "                        for lang in SMPLS]\n",
        "datasizes['Labels#'] = [len(set(tok for dts in ('trn', 'hld', 'dev', 'tst')\n",
        "                                for sent in DATA[lang][dts][1] for tok in sent))\n",
        "                        for lang in SMPLS]\n",
        "\n",
        "datasizes['Trn. Vocab#'] = [len(set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                    for tok in sent)) for lang in SMPLS]\n",
        "datasizes['Trn. Label#'] = [len(set(tok for sent in DATA[lang]['trn'][1]\n",
        "                                    for tok in sent)) for lang in SMPLS]\n",
        "\n",
        "datasizes['Ood. Vocab%'] = [len(set(tok for dts in ('hld', 'dev', 'tst')\n",
        "                                    for sent in DATA[lang][dts][0]\n",
        "                                    for tok in sent).difference(\n",
        "                                        set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                            for tok in sent)\n",
        "                                        )) for lang in SMPLS]\n",
        "datasizes['Ood. Label%'] = [len(set(tok for dts in ('hld', 'dev', 'tst')\n",
        "                                    for sent in DATA[lang][dts][1]\n",
        "                                    for tok in sent).difference(\n",
        "                                        set(tok for sent in DATA[lang]['trn'][0]\n",
        "                                            for tok in sent)\n",
        "                                        )) for lang in SMPLS]\n",
        "datasizes['Ood. Vocab%'] = 100 * datasizes['Ood. Vocab%'] / datasizes['Vocab#']\n",
        "datasizes['Ood. Label%'] = 100 * datasizes['Ood. Label%'] / datasizes['Labels#']\n",
        "\n",
        "datasizes "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Training</th>\n",
              "      <th>Held-out</th>\n",
              "      <th>Devel</th>\n",
              "      <th>Test</th>\n",
              "      <th>AllToks#</th>\n",
              "      <th>Vocab#</th>\n",
              "      <th>Labels#</th>\n",
              "      <th>Trn. Vocab#</th>\n",
              "      <th>Trn. Label#</th>\n",
              "      <th>Ood. Vocab%</th>\n",
              "      <th>Ood. Label%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Danish</td>\n",
              "      <td>175</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3421</td>\n",
              "      <td>3333</td>\n",
              "      <td>3280</td>\n",
              "      <td>2935</td>\n",
              "      <td>2886</td>\n",
              "      <td>11.941194</td>\n",
              "      <td>12.195122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>English</td>\n",
              "      <td>2006</td>\n",
              "      <td>354</td>\n",
              "      <td>590</td>\n",
              "      <td>0</td>\n",
              "      <td>13437</td>\n",
              "      <td>13127</td>\n",
              "      <td>12521</td>\n",
              "      <td>9589</td>\n",
              "      <td>9130</td>\n",
              "      <td>26.952083</td>\n",
              "      <td>27.561696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>482</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3285</td>\n",
              "      <td>3064</td>\n",
              "      <td>2833</td>\n",
              "      <td>2695</td>\n",
              "      <td>2504</td>\n",
              "      <td>12.043081</td>\n",
              "      <td>12.072008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>381</td>\n",
              "      <td>68</td>\n",
              "      <td>1588</td>\n",
              "      <td>0</td>\n",
              "      <td>9609</td>\n",
              "      <td>8886</td>\n",
              "      <td>8434</td>\n",
              "      <td>2110</td>\n",
              "      <td>2085</td>\n",
              "      <td>76.254783</td>\n",
              "      <td>75.492056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indonesian-English</td>\n",
              "      <td>420</td>\n",
              "      <td>75</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>6185</td>\n",
              "      <td>5754</td>\n",
              "      <td>5284</td>\n",
              "      <td>4256</td>\n",
              "      <td>3887</td>\n",
              "      <td>26.034063</td>\n",
              "      <td>27.346707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Italian</td>\n",
              "      <td>504</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4941</td>\n",
              "      <td>4619</td>\n",
              "      <td>4383</td>\n",
              "      <td>4046</td>\n",
              "      <td>3836</td>\n",
              "      <td>12.405283</td>\n",
              "      <td>12.616929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dutch</td>\n",
              "      <td>798</td>\n",
              "      <td>141</td>\n",
              "      <td>314</td>\n",
              "      <td>0</td>\n",
              "      <td>7885</td>\n",
              "      <td>6651</td>\n",
              "      <td>5394</td>\n",
              "      <td>3591</td>\n",
              "      <td>2902</td>\n",
              "      <td>46.008119</td>\n",
              "      <td>49.536522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Slovenian</td>\n",
              "      <td>3969</td>\n",
              "      <td>701</td>\n",
              "      <td>1557</td>\n",
              "      <td>0</td>\n",
              "      <td>17673</td>\n",
              "      <td>15963</td>\n",
              "      <td>14501</td>\n",
              "      <td>11229</td>\n",
              "      <td>10276</td>\n",
              "      <td>29.656080</td>\n",
              "      <td>30.315151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Serbian</td>\n",
              "      <td>3517</td>\n",
              "      <td>621</td>\n",
              "      <td>1327</td>\n",
              "      <td>0</td>\n",
              "      <td>20385</td>\n",
              "      <td>18658</td>\n",
              "      <td>17443</td>\n",
              "      <td>13705</td>\n",
              "      <td>12912</td>\n",
              "      <td>26.546254</td>\n",
              "      <td>27.019435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Turkish</td>\n",
              "      <td>484</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5837</td>\n",
              "      <td>4326</td>\n",
              "      <td>3992</td>\n",
              "      <td>3760</td>\n",
              "      <td>3501</td>\n",
              "      <td>13.083680</td>\n",
              "      <td>13.677355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Turkish-German</td>\n",
              "      <td>680</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7856</td>\n",
              "      <td>6156</td>\n",
              "      <td>5752</td>\n",
              "      <td>5514</td>\n",
              "      <td>5194</td>\n",
              "      <td>10.428850</td>\n",
              "      <td>10.639777</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Language  Training  ...  Ood. Vocab%  Ood. Label%\n",
              "0               Danish       175  ...    11.941194    12.195122\n",
              "1              English      2006  ...    26.952083    27.561696\n",
              "2              Spanish       482  ...    12.043081    12.072008\n",
              "3             Croatian       381  ...    76.254783    75.492056\n",
              "4   Indonesian-English       420  ...    26.034063    27.346707\n",
              "5              Italian       504  ...    12.405283    12.616929\n",
              "6                Dutch       798  ...    46.008119    49.536522\n",
              "7            Slovenian      3969  ...    29.656080    30.315151\n",
              "8              Serbian      3517  ...    26.546254    27.019435\n",
              "9              Turkish       484  ...    13.083680    13.677355\n",
              "10      Turkish-German       680  ...    10.428850    10.639777\n",
              "\n",
              "[11 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUyx0JVEMnj"
      },
      "source": [
        "### Sequence classification using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdjItmlUpj3k"
      },
      "source": [
        "##### Preprocessing\n",
        "\n",
        "In this step, we convert the sequence of tokens into an embedding matrix. \n",
        "This step relies on tokenizers and pre-trained models from ``huggingface``.\n",
        "This seperation of preprocessing should allow for training other classifiers \n",
        "than neural versions using ``scikit-learn`` or other packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qkRzWpGY-fW"
      },
      "source": [
        "for lang in SMPLS:\n",
        "  # load tokenizer model from ``huggingface``\n",
        "  for dts in DATA[lang]:\n",
        "    for inp, out in zip(DATA[lang][dts][0], DATA[lang][dts][1]):\n",
        "      tinp = []\n",
        "      tout = []\n",
        "      for intok, outok in zip(inp, out):\n",
        "        pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0rYO-eeEMy-"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5arwc2T385K"
      },
      "source": [
        "### Sequence classification using HMM models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtNuRD1539GR"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}