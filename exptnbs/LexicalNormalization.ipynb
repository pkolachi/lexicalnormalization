{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pkolachi/lexicalnormalization/blob/master/exptnbs/LexicalNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOoroVGYw3Mg"
   },
   "source": [
    "[WNUT21 Shared Task Website](http://noisy-text.github.io/2021/multi-lexnorm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzRtzY3ApXhe"
   },
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywhY3ZLXJs0P",
    "outputId": "636b8148-d9c3-4f86-855e-84d73ef71c88"
   },
   "outputs": [],
   "source": [
    "# We no longer clone the github repository. Instead this notebook\n",
    "# is part of the repository itself\n",
    "# !git clone https://github.com/pkolachi/lexicalnormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user -U pandas==1.1.5\n",
    "%pip install --user -U scikit-learn==0.22.2.post1\n",
    "%pip install --user -U sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTkdFPXmKBCH"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"multilexnorm\"\n",
    "LANGS = {\n",
    "    \"da\": \"Danish\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"iden\": \"Indonesian-English\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"sr\": \"Serbian\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"trde\": \"Turkish-German\",\n",
    "}\n",
    "SMPLS = LANGS.keys()\n",
    "EMPTY_TOKEN = \"+-#EMPTOK#-+\"\n",
    "EMPTY_LABEL = \"+-#MERGE#-+\"  #''\n",
    "PAD_TOKEN = \"+-#NULL#-+\"\n",
    "PAD_LABEL = \"+-#DROP#-+\"\n",
    "TST_RATIO = 0.15  # use 15% of training data as held-out data for evaluation\n",
    "CVFOLDS = 4  # use 4-folds for cross-fold training throughout experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSzppLsjpdql"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTSBr-AMNXzf"
   },
   "outputs": [],
   "source": [
    "def load_data(inpfile, empty_label=\"\"):\n",
    "    with open(inpfile) as inf:\n",
    "        # break lines into sentence blocks\n",
    "        snb = (list(it.takewhile(lambda lne: lne.strip(), inf)) for _ in it.count(1))\n",
    "        # deal with errors in file format especially turkish\n",
    "        snc = it.dropwhile(lambda snt: len(snt) == 0, snb)\n",
    "        # terminate this infinite stream\n",
    "        snd = it.takewhile(lambda snt: len(snt) > 0, snc)\n",
    "        # split into fields\n",
    "        crs = ([t.strip(\"\\n\").split(\"\\t\", 1) for t in s] for s in snd)\n",
    "        if empty_label:\n",
    "            crp = [\n",
    "                [\n",
    "                    (tok[0], tok[1] if len(tok) > 1 and tok[1].strip() else empty_label)\n",
    "                    for tok in sent\n",
    "                ]\n",
    "                for sent in crs\n",
    "            ]\n",
    "        else:\n",
    "            crp = list(crs)\n",
    "        return crp\n",
    "\n",
    "\n",
    "# remove sentences that do not follow the expected format\n",
    "sanitize_crps = lambda sent: all(len(fields) == 2 for fields in sent)\n",
    "# get input from tuple (raw sentences)\n",
    "get_rawtokens = lambda sent: list(map(itemgetter(0), sent))\n",
    "# get output/labels from tuple (normalized sentences)\n",
    "get_nrmtokens = lambda sent: list(map(itemgetter(1), sent))\n",
    "\n",
    "DATA = defaultdict(lambda: defaultdict(lambda: ([], [])))\n",
    "for lang in SMPLS:\n",
    "    datadir = os.path.join(\"..\", REPO_NAME, \"data\", lang)\n",
    "    trnfile = os.path.join(datadir, \"train.norm\")\n",
    "    devfile = os.path.join(datadir, \"dev.norm\")\n",
    "    tstfile = os.path.join(datadir, \"test.norm\")\n",
    "    for dts, dtf in [(\"fulltrn\", trnfile), (\"dev\", devfile), (\"tst\", tstfile)]:\n",
    "        if os.path.isdir(datadir) and os.path.isfile(dtf):\n",
    "            ocrp = list(load_data(dtf, empty_label=EMPTY_LABEL))\n",
    "            # sanitize corpus to make sure\n",
    "            fcrp = list(filter(sanitize_crps, ocrp))\n",
    "            if len(ocrp) != len(fcrp):\n",
    "                print(f\"Removed {len(ocrp) - len(fcrp)} sentences from {dtf}\")\n",
    "            X = list(map(get_rawtokens, fcrp))\n",
    "            Y = list(map(get_nrmtokens, fcrp))\n",
    "            DATA[lang][dts] = (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkCFpXbrwD5U"
   },
   "outputs": [],
   "source": [
    "# %rm -rf $REPO_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmm52m_3Gzo"
   },
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4J-9JaRXMOX"
   },
   "outputs": [],
   "source": [
    "for lang in SMPLS:\n",
    "    if \"fulltrn\" in DATA[lang]:\n",
    "        trn_x, hld_x, trn_y, hld_y = train_test_split(\n",
    "            DATA[lang][\"fulltrn\"][0],\n",
    "            DATA[lang][\"fulltrn\"][1],\n",
    "            test_size=TST_RATIO,\n",
    "            random_state=0,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        DATA[lang][\"trn\"] = (trn_x, trn_y)\n",
    "        DATA[lang][\"hld\"] = (hld_x, hld_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "7nz69Y8xwD5U",
    "outputId": "4a0e61cd-140a-4446-800c-a10da5d0f924"
   },
   "outputs": [],
   "source": [
    "columns = [\"Language\", \"Training\", \"Held-out\", \"Devel\", \"Test\"]\n",
    "datasizes = [\n",
    "    [LANGS[lang]] + [len(DATA[lang][crp][0]) for crp in (\"trn\", \"hld\", \"dev\", \"tst\")]\n",
    "    for lang in SMPLS\n",
    "]\n",
    "datasizes = pd.DataFrame.from_records(datasizes, columns=columns)\n",
    "\n",
    "datasizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "PC8E_x4F3TTu",
    "outputId": "fd2468df-6be4-410d-b0b1-904053b20d49"
   },
   "outputs": [],
   "source": [
    "datasizes[\"AllToks#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for dts in (\"trn\", \"hld\", \"dev\", \"tst\")\n",
    "            for typ in (0, 1)\n",
    "            for sent in DATA[lang][dts][typ]\n",
    "            for tok in sent\n",
    "        }\n",
    "    )\n",
    "    for lang in SMPLS\n",
    "]\n",
    "\n",
    "datasizes[\"Vocab#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for dts in (\"trn\", \"hld\", \"dev\", \"tst\")\n",
    "            for sent in DATA[lang][dts][0]\n",
    "            for tok in sent\n",
    "        }\n",
    "    )\n",
    "    for lang in SMPLS\n",
    "]\n",
    "datasizes[\"Labels#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for dts in (\"trn\", \"hld\", \"dev\", \"tst\")\n",
    "            for sent in DATA[lang][dts][1]\n",
    "            for tok in sent\n",
    "        }\n",
    "    )\n",
    "    for lang in SMPLS\n",
    "]\n",
    "\n",
    "datasizes[\"Trn. Vocab#\"] = [\n",
    "    len({tok for sent in DATA[lang][\"trn\"][0] for tok in sent}) for lang in SMPLS\n",
    "]\n",
    "datasizes[\"Trn. Label#\"] = [\n",
    "    len({tok for sent in DATA[lang][\"trn\"][1] for tok in sent}) for lang in SMPLS\n",
    "]\n",
    "\n",
    "datasizes[\"Ood. Vocab%\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for dts in (\"hld\", \"dev\", \"tst\")\n",
    "            for sent in DATA[lang][dts][0]\n",
    "            for tok in sent\n",
    "        }.difference({tok for sent in DATA[lang][\"trn\"][0] for tok in sent})\n",
    "    )\n",
    "    for lang in SMPLS\n",
    "]\n",
    "datasizes[\"Ood. Label%\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for dts in (\"hld\", \"dev\", \"tst\")\n",
    "            for sent in DATA[lang][dts][1]\n",
    "            for tok in sent\n",
    "        }.difference({tok for sent in DATA[lang][\"trn\"][0] for tok in sent})\n",
    "    )\n",
    "    for lang in SMPLS\n",
    "]\n",
    "datasizes[\"Ood. Vocab%\"] = 100 * datasizes[\"Ood. Vocab%\"] / datasizes[\"Vocab#\"]\n",
    "datasizes[\"Ood. Label%\"] = 100 * datasizes[\"Ood. Label%\"] / datasizes[\"Labels#\"]\n",
    "\n",
    "datasizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRbS1VJ396r2"
   },
   "source": [
    "### Task Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omrn2mZv7bT9"
   },
   "outputs": [],
   "source": [
    "FOLDS = defaultdict(list)\n",
    "kcvs = RepeatedKFold(n_splits=CVFOLDS, n_repeats=5, random_state=0)\n",
    "for lang in SMPLS:\n",
    "    FOLDS[lang].extend(kcvs.split(DATA[lang][\"trn\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwPn_NHdKG6g"
   },
   "outputs": [],
   "source": [
    "# Leave-As-Is i.e. return input as output\n",
    "class LAI(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, pad_tok=PAD_TOKEN, pad_lbl=PAD_LABEL):\n",
    "        self.__pad_tok = pad_tok\n",
    "        self.__pad_lbl = pad_lbl\n",
    "        self.__scores = defaultdict(float)\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        max_len = max(it.chain((len(seq) for seq in X), (len(seq) for seq in Y)))\n",
    "        # X = np.asarray([seq + [self.__pad_tok]*(max_len-len(seq)) for seq in X], dtype=str)\n",
    "        # Y = np.asarray([seq + [self.__pad_lbl]*(max_len-len(seq)) for seq in Y], dtype=str)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        max_len = max(len(seq) for seq in X)\n",
    "        # X = np.asarray([seq + [self.__pad_tok]*(max_len-len(seq)) for seq in X], dtype=str)\n",
    "        return X\n",
    "\n",
    "    def score(self, X, Y, ignoreCase=False):\n",
    "        prdY = self.predict(X)\n",
    "        zipS = ((inp, out, oup) for inp, out, oup in zip(X, Y, prdY))\n",
    "        # eliminate instances if lengths do not match\n",
    "        zipF = (seq for seq in zipS if len(seq[1]) == len(seq[2]))\n",
    "        tokS = ((rawW, gldW, prdW) for seq in zipF for rawW, gldW, prdW in zip(*seq))\n",
    "        correct, changed, total = 0, 0, 0\n",
    "        for rawW, gldW, prdW in tokS:\n",
    "            total += 1\n",
    "            if ignoreCase:\n",
    "                rawW = rawW.lower()\n",
    "                gldW = gldW.lower()\n",
    "                prdW = prdW.lower()\n",
    "            if rawW != gldW:\n",
    "                changed += 1\n",
    "            if gldW == prdW:\n",
    "                correct += 1\n",
    "        # evaluation used in the shared task\n",
    "        self.__scores[\"accuracy\"] = correct / total\n",
    "        self.__scores[\"lai\"] = (total - changed) / total\n",
    "        if self.__scores[\"lai\"] == 1:\n",
    "            self.__scores[\"err\"] = 0\n",
    "        else:\n",
    "            self.__scores[\"err\"] = (\n",
    "                self.__scores[\"accuracy\"] - self.__scores[\"lai\"]\n",
    "            ) / (1 - self.__scores[\"lai\"])\n",
    "        return self.__scores[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTZOImW997A-"
   },
   "outputs": [],
   "source": [
    "# Most-Frequent-Replacement\n",
    "class MFR(LAI):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.__counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        super().fit(X, Y)\n",
    "        replacements = (\n",
    "            (itok, otok) for iseg, oseg in zip(X, Y) for itok, otok in zip(iseg, oseg)\n",
    "        )\n",
    "        for inp, rpl in replacements:\n",
    "            self.__counts[inp][rpl] += 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = super().predict(X)\n",
    "        prdY = []\n",
    "        for iseg in X:\n",
    "            oseg = []\n",
    "            for itok in iseg:\n",
    "                lns = self.__counts[itok]\n",
    "                if len(lns) == 0:\n",
    "                    oseg.append(itok)\n",
    "                else:\n",
    "                    oseg.append(sorted(lns.items(), key=itemgetter(1))[0][0])\n",
    "            prdY.append(oseg)\n",
    "        return prdY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFV_O4WAFRoM",
    "outputId": "dc215fa1-9c58-441c-e223-1309e3ae23fb"
   },
   "outputs": [],
   "source": [
    "for lang in SMPLS:\n",
    "    cvres = cross_validate(\n",
    "        LAI(),\n",
    "        DATA[lang][\"trn\"][0],\n",
    "        DATA[lang][\"trn\"][1],\n",
    "        cv=FOLDS[lang],\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    print(lang, sum(acc for acc in cvres[\"test_score\"]) / len(cvres[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZUyx0JVEMnj"
   },
   "source": [
    "### Sequence classification using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdjItmlUpj3k"
   },
   "source": [
    "##### Preprocessing\n",
    "\n",
    "In this step, we convert the sequence of tokens into an embedding matrix. \n",
    "This step relies on tokenizers and pre-trained models from ``huggingface``.\n",
    "This seperation of preprocessing should allow for training other classifiers \n",
    "than neural versions using ``scikit-learn`` or other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qkRzWpGY-fW"
   },
   "outputs": [],
   "source": [
    "for lang in SMPLS:\n",
    "    # load tokenizer model from ``huggingface``\n",
    "    for dts in DATA[lang]:\n",
    "        for inp, out in zip(DATA[lang][dts][0], DATA[lang][dts][1]):\n",
    "            tinp = []\n",
    "            tout = []\n",
    "            for intok, outok in zip(inp, out):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0rYO-eeEMy-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5arwc2T385K"
   },
   "source": [
    "### Sequence classification using HMM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtNuRD1539GR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LexicalNormalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
