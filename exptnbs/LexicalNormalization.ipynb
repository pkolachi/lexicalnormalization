{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalNormalization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkolachi/lexicalnormalization/blob/master/exptnbs/LexicalNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOoroVGYw3Mg"
      },
      "source": [
        "[WNUT21 Shared Task Website](http://noisy-text.github.io/2021/multi-lexnorm.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRtzY3ApXhe"
      },
      "source": [
        "### Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywhY3ZLXJs0P",
        "outputId": "69084dc5-23cf-4809-a213-5cf393cfd476"
      },
      "source": [
        "!git clone https://github.com/pkolachi/lexicalnormalization\n",
        "%pip install --user -U pandas==1.1.5\n",
        "%pip install --user -U scikit-learn==0.22.2.post1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lexicalnormalization' already exists and is not an empty directory.\n",
            "Requirement already up-to-date: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkdFPXmKBCH"
      },
      "source": [
        "REPO_NAME = 'lexicalnormalization'\n",
        "LANGS = {'da': 'Danish',\n",
        "         'en': 'English',\n",
        "         'es': 'Spanish',\n",
        "         'hr': 'Croatian',\n",
        "         'iden': 'Indonesian-English',\n",
        "         'it': 'Italian',\n",
        "         'nl': 'Dutch',\n",
        "         'sl': 'Slovenian',\n",
        "         'sr': 'Serbian',\n",
        "         'tr': 'Turkish',\n",
        "         'trde': 'Turkish-German',\n",
        "         }\n",
        "SMPLS = LANGS.keys()\n",
        "EMPTY_LABEL = '+-#MERGE#-+' #''\n",
        "PAD_LABEL   = '+-#DROP#-+'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzppLsjpdql"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTSBr-AMNXzf"
      },
      "source": [
        "from collections import defaultdict \n",
        "from operator import itemgetter\n",
        "import os.path \n",
        "import pandas as pd\n",
        "\n",
        "def load_data(inpfile, empty_label=''): \n",
        "  with open(inpfile) as ins:\n",
        "    sent = []\n",
        "    for lne in ins:\n",
        "      if not lne.strip():\n",
        "        yield sent\n",
        "        sent = []\n",
        "      else:\n",
        "        fields = tuple(lne.strip('\\n').split('\\t', 1))\n",
        "        if len(fields) < 2:\n",
        "          fields = (fields, empty_label)\n",
        "        elif not fields[1].strip() and empty_label:\n",
        "          fields = (fields[0], empty_label)\n",
        "        sent.append(fields)\n",
        "\n",
        "sanitize_crps = lambda sent: all(len(fields) == 2 for fields in sent)\n",
        "get_rawtokens = lambda sent: list(map(itemgetter(0), sent)) # load input (un-normalized sentences)\n",
        "get_nrmtokens = lambda sent: list(map(itemgetter(1), sent)) # load output (normalized sentences)\n",
        "\n",
        "DATA = defaultdict(lambda: defaultdict(lambda: ([], [])))\n",
        "for lang in SMPLS:\n",
        "  datadir = os.path.join(REPO_NAME, 'data', lang)\n",
        "  trnfile = os.path.join(datadir, 'train.norm')\n",
        "  devfile = os.path.join(datadir, 'dev.norm')\n",
        "  tstfile = os.path.join(datadir, 'test.norm')\n",
        "  for dts, dtf in [('fulltrn', trnfile), ('dev', devfile), ('tst', tstfile)]:\n",
        "    if os.path.isdir(datadir) and os.path.isfile(dtf):\n",
        "      ocrp = list(load_data(dtf, empty_label=EMPTY_LABEL))\n",
        "      # sanitize corpus to make sure\n",
        "      fcrp = list(filter(sanitize_crps, ocrp))\n",
        "      if len(ocrp) != len(fcrp): print(\"Removed {0} sentences from {1}\".format(len(ocrp)-len(fcrp), dtf))\n",
        "      X = list(map(get_rawtokens, fcrp))  \n",
        "      Y = list(map(get_nrmtokens, fcrp)) \n",
        "      DATA[lang][dts] = (X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4J-9JaRXMOX"
      },
      "source": [
        "TST_RATIO = 0.15\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "for lang in SMPLS:\n",
        "  if 'fulltrn' in DATA[lang]:\n",
        "    trn_x, hld_x, trn_y, hld_y = train_test_split(DATA[lang]['fulltrn'][0], \n",
        "                                                  DATA[lang]['fulltrn'][1], \n",
        "                                                  test_size=TST_RATIO, \n",
        "                                                  random_state=0, \n",
        "                                                  shuffle=False)\n",
        "    DATA[lang]['trn'] = (trn_x, trn_y)\n",
        "    DATA[lang]['hld'] = (hld_x, hld_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "7nz69Y8xwD5U",
        "outputId": "976cb5d0-76c4-472e-f246-286f222d7220"
      },
      "source": [
        "columns = ['Language', 'Training', 'Held-out', 'Development', 'Testing']\n",
        "datasizes = [[LANGS[lang]]+[len(DATA[lang][crp][0]) \n",
        "                     for crp in ('trn', 'hld', 'dev', 'tst')] \n",
        "             for lang in SMPLS]\n",
        "datasizes = pd.DataFrame.from_records(datasizes, columns=columns)\n",
        "\n",
        "datasizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Training</th>\n",
              "      <th>Held-out</th>\n",
              "      <th>Development</th>\n",
              "      <th>Testing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Danish</td>\n",
              "      <td>175</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>English</td>\n",
              "      <td>2006</td>\n",
              "      <td>354</td>\n",
              "      <td>590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spanish</td>\n",
              "      <td>482</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Croatian</td>\n",
              "      <td>4049</td>\n",
              "      <td>715</td>\n",
              "      <td>1588</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indonesian-English</td>\n",
              "      <td>420</td>\n",
              "      <td>75</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Italian</td>\n",
              "      <td>504</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dutch</td>\n",
              "      <td>797</td>\n",
              "      <td>141</td>\n",
              "      <td>313</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Slovenian</td>\n",
              "      <td>3969</td>\n",
              "      <td>701</td>\n",
              "      <td>1557</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Serbian</td>\n",
              "      <td>3517</td>\n",
              "      <td>621</td>\n",
              "      <td>1381</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Turkish</td>\n",
              "      <td>486</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Turkish-German</td>\n",
              "      <td>680</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Language  Training  Held-out  Development  Testing\n",
              "0               Danish       175        32            0        0\n",
              "1              English      2006       354          590        0\n",
              "2              Spanish       482        86            0        0\n",
              "3             Croatian      4049       715         1588        0\n",
              "4   Indonesian-English       420        75          165        0\n",
              "5              Italian       504        89            0        0\n",
              "6                Dutch       797       141          313        0\n",
              "7            Slovenian      3969       701         1557        0\n",
              "8              Serbian      3517       621         1381        0\n",
              "9              Turkish       486        86            0        0\n",
              "10      Turkish-German       680       120            0        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkCFpXbrwD5U"
      },
      "source": [
        "%rm -rf $REPO_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUyx0JVEMnj"
      },
      "source": [
        "### Sequence classification using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdjItmlUpj3k"
      },
      "source": [
        "##### Preprocessing\n",
        "In this step, we convert the sequence of tokens into an embedding matrix. \n",
        "This step relies on tokenizers and pre-trained models from ``huggingface``.\n",
        "This seperation of preprocessing should allow for training other classifiers \n",
        "than neural versions using ``sklearn`` or other packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qkRzWpGY-fW"
      },
      "source": [
        "for lang in SMPLS:\n",
        "  # load tokenizer model from ``huggingface``\n",
        "  for dts in DATA[lang]:\n",
        "    for inp, out in zip(DATA[lang][dts][0], DATA[lang][dts][1]):\n",
        "      tinp = []\n",
        "      tout = []\n",
        "      for intok, outok in zip(inp, out):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0rYO-eeEMy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5ec7dc-8c1d-43d0-c993-24710d351209"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5arwc2T385K"
      },
      "source": [
        "### Sequence classification using HMM models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtNuRD1539GR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}