{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOoroVGYw3Mg"
   },
   "source": [
    "[WNUT21 Shared Task Website](http://noisy-text.github.io/2021/multi-lexnorm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzRtzY3ApXhe"
   },
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywhY3ZLXJs0P",
    "outputId": "636b8148-d9c3-4f86-855e-84d73ef71c88"
   },
   "outputs": [],
   "source": [
    "# We no longer clone the github repository. Instead this notebook\n",
    "# is part of the repository itself\n",
    "# !git clone https://github.com/pkolachi/lexicalnormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user -U pandas==1.1.5\n",
    "# %pip install --user -U scikit-learn==0.22.2.post1\n",
    "# %pip install --user -U sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os.path\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/lexnorm/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GTkdFPXmKBCH"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"multilexnorm\"\n",
    "LANGS = {\n",
    "    \"da\": \"Danish\",\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"hr\": \"Croatian\",\n",
    "    \"iden\": \"Indonesian-English\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"nl\": \"Dutch\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"sr\": \"Serbian\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"trde\": \"Turkish-German\",\n",
    "}\n",
    "SAMPLE_LANGS = [\"en\", \"es\", \"it\", \"da\"]\n",
    "TST_RATIO = 0.15  # use 15% of training data as held-out data for evaluation\n",
    "CVFOLDS = 4  # use 4-folds for cross-fold training throughout experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSzppLsjpdql"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AlignedParSent:\n",
    "    src: str = \"\"\n",
    "    tgt: str = \"\"\n",
    "    alignments: tuple = ()\n",
    "    slang: str = \"en\"\n",
    "    tlang: str = slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block2sent(block):\n",
    "    src_sent, tgt_sent = [], []\n",
    "    alignments = []\n",
    "    src_idx, tgt_idx = 0, 0\n",
    "    for entry in block:\n",
    "        src, tgt = entry.split(\"\\t\", 1)\n",
    "        src_toks, tgt_toks = src.split(), tgt.split()\n",
    "        src_tok_ids = range(src_idx, src_idx + len(src_toks))\n",
    "        tgt_tok_ids = range(tgt_idx, tgt_idx + len(tgt_toks))\n",
    "        src_sent.extend(src_toks)\n",
    "        tgt_sent.extend(tgt_toks)\n",
    "        alignments.extend(it.product(src_tok_ids, tgt_tok_ids))\n",
    "        src_idx += len(src_toks)\n",
    "        tgt_idx += len(tgt_toks)\n",
    "    par_sent = AlignedParSent(\n",
    "        \" \".join(src_sent), \" \".join(tgt_sent), tuple(alignments)\n",
    "    )\n",
    "    return par_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VTSBr-AMNXzf"
   },
   "outputs": [],
   "source": [
    "def load_data(inpfile):\n",
    "    corpus = []\n",
    "    with open(inpfile) as infile:\n",
    "        # break lines into sentence blocks\n",
    "        block = []\n",
    "        for line in infile:\n",
    "            if not line.strip() and len(block):\n",
    "                corpus.append(block2sent(block))\n",
    "                block = []\n",
    "            elif not line.strip():\n",
    "                continue\n",
    "            else:\n",
    "                block.append(line.strip(\"\\n\"))\n",
    "        if len(block):\n",
    "            corpus.append(block2sent(block))\n",
    "            block = []\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_proper(sent):\n",
    "    return (\n",
    "        len(sent.alignments) != 0\n",
    "        and max(map(itemgetter(0), sent.alignments)) < len(sent.src.split())\n",
    "        and max(map(itemgetter(1), sent.alignments)) < len(sent.tgt.split())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = defaultdict(lambda: defaultdict(list))\n",
    "for lang in SAMPLE_LANGS:\n",
    "    data_directory = os.path.join(\"..\", REPO_NAME, \"data\", lang)\n",
    "    train_file = os.path.join(data_directory, \"train.norm\")\n",
    "    dev_file = os.path.join(data_directory, \"dev.norm\")\n",
    "    test_file = os.path.join(data_directory, \"test.norm\")\n",
    "    for dts, dtf in [\n",
    "        (\"fulltrn\", train_file),\n",
    "        (\"dev\", dev_file),\n",
    "        (\"tst\", test_file),\n",
    "    ]:\n",
    "        if os.path.isdir(data_directory) and os.path.isfile(dtf):\n",
    "            corpus_all = list(load_data(dtf))\n",
    "            # sanitize corpus to make sure\n",
    "            corpus_filtered = list(filter(is_proper, corpus_all))\n",
    "            if len(corpus_all) != len(corpus_filtered):\n",
    "                print(\n",
    "                    f\"Removed {len(corpus_all) - len(corpus_filtered)} sentences from {dtf}\"\n",
    "                )\n",
    "            DATA[lang][dts] = corpus_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V4J-9JaRXMOX"
   },
   "outputs": [],
   "source": [
    "for lang in SAMPLE_LANGS:\n",
    "    if \"fulltrn\" in DATA[lang]:\n",
    "        corpus_train, corpus_heldout = train_test_split(\n",
    "            DATA[lang][\"fulltrn\"],\n",
    "            test_size=TST_RATIO,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        DATA[lang][\"trn\"] = corpus_train\n",
    "        DATA[lang][\"hld\"] = corpus_heldout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DkCFpXbrwD5U"
   },
   "outputs": [],
   "source": [
    "# %rm -rf $REPO_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmm52m_3Gzo"
   },
   "source": [
    "### Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "7nz69Y8xwD5U",
    "outputId": "4a0e61cd-140a-4446-800c-a10da5d0f924"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Language   | Dataset     |   #Instances |\n",
       "|:-----------|:------------|-------------:|\n",
       "| English    | Training    |         2006 |\n",
       "| English    | Held-out    |          354 |\n",
       "| English    | Development |          590 |\n",
       "| English    | Test        |         1967 |\n",
       "| Spanish    | Training    |          482 |\n",
       "| Spanish    | Held-out    |           86 |\n",
       "| Spanish    | Development |            0 |\n",
       "| Spanish    | Test        |          531 |\n",
       "| Italian    | Training    |          504 |\n",
       "| Italian    | Held-out    |           89 |\n",
       "| Italian    | Development |            0 |\n",
       "| Italian    | Test        |          100 |\n",
       "| Danish     | Training    |          611 |\n",
       "| Danish     | Held-out    |          108 |\n",
       "| Danish     | Development |            0 |\n",
       "| Danish     | Test        |          181 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = [\"Language\", \"Dataset\", \"#Instances\"]\n",
    "partition_keys = {\n",
    "    \"trn\": \"Training\",\n",
    "    \"hld\": \"Held-out\",\n",
    "    \"dev\": \"Development\",\n",
    "    \"tst\": \"Test\",\n",
    "}\n",
    "datasizes = []\n",
    "for lang in SAMPLE_LANGS:\n",
    "    for data_set in (\"trn\", \"hld\", \"dev\", \"tst\"):\n",
    "        datasizes.append(\n",
    "            [LANGS[lang], partition_keys[data_set], len(DATA[lang][data_set])]\n",
    "        )\n",
    "\n",
    "DATA_STATS = pd.DataFrame.from_records(datasizes, columns=columns)\n",
    "\n",
    "display(Markdown(DATA_STATS.to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "PC8E_x4F3TTu",
    "outputId": "fd2468df-6be4-410d-b0b1-904053b20d49"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Language   | Dataset     |   #Instances |   All Vocab# |   Src. Vocab# |   Tgt. Vocab# |   Oov. src. Vocab# |   Oov. tgt. Vocab# |   Oov. src. Vocab% |   Oov. tgt. Vocab% |\n",
       "|:-----------|:------------|-------------:|-------------:|--------------:|--------------:|-------------------:|-------------------:|-------------------:|-------------------:|\n",
       "| English    | Training    |         2006 |        19461 |          9589 |          9049 |                nan |                nan |          nan       |          nan       |\n",
       "| English    | Held-out    |          354 |        19461 |          2494 |          2369 |               1337 |               1255 |            6.87015 |            6.4488  |\n",
       "| English    | Development |          590 |        19461 |          3906 |          3714 |               2280 |               2158 |           11.7157  |           11.0888  |\n",
       "| English    | Test        |         1967 |        19461 |          9400 |          8867 |               6620 |               6256 |           34.0168  |           32.1463  |\n",
       "| Spanish    | Training    |          482 |         5680 |          2693 |          2502 |                nan |                nan |          nan       |          nan       |\n",
       "| Spanish    | Held-out    |           86 |         5680 |           614 |           585 |                369 |                329 |            6.49648 |            5.79225 |\n",
       "| Spanish    | Development |          nan |         5680 |           nan |           nan |                nan |                nan |          nan       |          nan       |\n",
       "| Spanish    | Test        |          531 |         5680 |          2991 |          2784 |               2332 |               2116 |           41.0563  |           37.2535  |\n",
       "| Italian    | Training    |          504 |         5441 |          4046 |          3831 |                nan |                nan |          nan       |          nan       |\n",
       "| Italian    | Held-out    |           89 |         5441 |          1038 |          1012 |                573 |                542 |           10.5312  |            9.9614  |\n",
       "| Italian    | Development |          nan |         5441 |           nan |           nan |                nan |                nan |          nan       |          nan       |\n",
       "| Italian    | Test        |          100 |         5441 |          1004 |           970 |                523 |                491 |            9.6122  |            9.02408 |\n",
       "| Danish     | Training    |          611 |         6100 |          3454 |          3175 |                nan |                nan |          nan       |          nan       |\n",
       "| Danish     | Held-out    |          108 |         6100 |          1993 |          1959 |               1404 |               1316 |           23.0164  |           21.5738  |\n",
       "| Danish     | Development |          nan |         6100 |           nan |           nan |                nan |                nan |          nan       |          nan       |\n",
       "| Danish     | Test        |          181 |         6100 |          1413 |          1348 |                839 |                773 |           13.7541  |           12.6721  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PARTITIONS = (\"trn\", \"hld\", \"dev\", \"tst\")\n",
    "\n",
    "DATA_STATS[\"All Vocab#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok\n",
    "            for data_part in DATA_PARTITIONS\n",
    "            for item in DATA[lang][data_part]\n",
    "            for sent in (item.src, item.tgt)\n",
    "            for tok in sent.split()\n",
    "        }\n",
    "    )\n",
    "    for lang in SAMPLE_LANGS\n",
    "    for data_part in DATA_PARTITIONS\n",
    "]\n",
    "\n",
    "DATA_STATS[\"Src. Vocab#\"] = [\n",
    "    len({tok for item in DATA[lang][data_part] for tok in item.src.split()})\n",
    "    for lang in SAMPLE_LANGS\n",
    "    for data_part in DATA_PARTITIONS\n",
    "]\n",
    "\n",
    "DATA_STATS[\"Tgt. Vocab#\"] = [\n",
    "    len({tok for item in DATA[lang][data_part] for tok in item.tgt.split()})\n",
    "    for lang in SAMPLE_LANGS\n",
    "    for data_part in DATA_PARTITIONS\n",
    "]\n",
    "\n",
    "DATA_STATS[\"Oov. src. Vocab#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok for item in DATA[lang][data_part] for tok in item.src.split()\n",
    "        }.difference(\n",
    "            {tok for item in DATA[lang][\"trn\"] for tok in item.src.split()}\n",
    "        )\n",
    "    )\n",
    "    for lang in SAMPLE_LANGS\n",
    "    for data_part in DATA_PARTITIONS\n",
    "]\n",
    "\n",
    "DATA_STATS[\"Oov. tgt. Vocab#\"] = [\n",
    "    len(\n",
    "        {\n",
    "            tok for item in DATA[lang][data_part] for tok in item.tgt.split()\n",
    "        }.difference(\n",
    "            {tok for item in DATA[lang][\"trn\"] for tok in item.tgt.split()}\n",
    "        )\n",
    "    )\n",
    "    for lang in SAMPLE_LANGS\n",
    "    for data_part in DATA_PARTITIONS\n",
    "]\n",
    "\n",
    "# Percentage of out-of-vocabulary tokens in noisy sentences\n",
    "DATA_STATS[\"Oov. src. Vocab%\"] = (\n",
    "    100 * DATA_STATS[\"Oov. src. Vocab#\"] / DATA_STATS[\"All Vocab#\"]\n",
    ")\n",
    "# Percentage of out-of-vocabulary tokens in normalized sentences \n",
    "DATA_STATS[\"Oov. tgt. Vocab%\"] = (\n",
    "    100 * DATA_STATS[\"Oov. tgt. Vocab#\"] / DATA_STATS[\"All Vocab#\"]\n",
    ")\n",
    "# replace all zero values with nan\n",
    "DATA_STATS.replace(0, np.nan, inplace=True)\n",
    "\n",
    "display(Markdown(DATA_STATS.to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LexicalNormalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
